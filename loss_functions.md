## Функции потерь
### logloss и функция правдоподобия
Перевод [What is Log Loss?](https://www.kaggle.com/dansbecker/what-is-log-loss)
#### Введение
logloss - это наиболее важная метрика классифкации, основанная на вероятности

Сложно интерпретировать сырые значения logloss, но это все же хорошая метрика для сравнения моделей. Для любой проблемы классифкации, чем ниже значение logloss, тем лучше предсказание

logloss - это переворот функции правдоподобия. По сути, `logloss это -1 * логарифм функции правдоподобия`.

Функция правдоподобия отвечает на вопрос "Насколько вероятным для модели был наблюдаемый набор результатов". Такое объяснение несколько сбивает с толку, и лучше понять его смылс поможет пример.

#### Пример
Скажем, модель предсказывает [0.8, 0.4, 0.1] для продажи трех домов. Первые два дома были проданы, а третий - нет. Таким образом, действительные результаты продажи домов можно представить численно так: [1, 1, 0]

Давайте пройдемся по этим прогнозам по одному за раз, итеративно вычисляя функцию правдоподобия.
Первый дом был продан и модель сказала, что это было на 80% вероятно. Таким образом, функция правдоподобия после взгляда на первое предсказание равна 0.8
Второй дом был продан и модель сказала, что на это была 40% вероятность. Согласно правилу вычисления вероятности множества независимых событий - их совместная вероятность - это произведение их индивидуальных вероятностей. Таким образом, мы можем получить совместное правдоподобие для двух предсказаний перемножением их вероятностей: 0.8*0.4 = 0.32

Теперь рассмотрим третье предсказание. Третий дом не был продан. Модель прогнозировала, что вероятность продать этот дом была 10%. Это означает, что с вероятностью 90% он не должен был быть продан. Таким образом, наблюдаемый результат что дом не был продан был вероятен с вероятностью 90%. Умножим предыдущий результат 0.32*0.9.

Так мы рассмотрели все прогнозы нашей модели. Каждый раз мы находили вероятность, связанную с фактическим результатом, и умножали её на предыдущий результат. Это и есть правдоподобие.

#### От правдоподобия к logloss
Каждое предсказание находится в интервале от 0 до 1. Если перемножать дастаточное много числе в этом диапазоне, то результат получится настолько малым, что компьютеры не смогут его отслеживать. Поэтому воспользуемся вычислительным трюком и будем отслеживать логарифм правдоподобия. Это приведет к диапазону, который легко отслеживать. А на -1 умножаем для того, чтобы сохранить общую договорённость о том, что чем меньше бал (фукнции потерь) - тем лучше.
